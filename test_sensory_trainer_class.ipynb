{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d59a136",
   "metadata": {},
   "source": [
    "# Sensory train class\n",
    "I've made many versions for different setup and different data split and interpolation. Now it's time to merge them... but with some differences.  \n",
    "It will be subclass of tensorflow model (or Keras model?) with methods & initialization; \n",
    "- interpolation : interpolate missing values or not\n",
    "- limit dates range : to test & compile..    \n",
    "- change dataset: I have 3 different version of same dataset. Finally I have to use them...\n",
    "- sequence & model: unlike previous notebooks, it does not make long sequences from none-missing dataframe anymore. instead, it will just try to make sequence from existing rows.  \n",
    "- sensor positions: It will come with options to one-hot encode position or not.\n",
    "- normalization of mean, standard deviation\n",
    "- ready to be used by Weight and Bias sweep agent\n",
    "\n",
    "## Why does it changed (from previous version)\n",
    "previously I received 80/20 and 50/50 split dataset from our . But I made merged version instead of using that data. I have merged, interpolated missing values and merged 5 points data measured at same time. But all this efforts are not adquete to apply to the new dataset, which is separated randomly without considering date&time at all. So I had to do it everything again to compare new results with previous dataset. I cannot apply the previous method so I have to make new method and apply it to the both dataset. So I feel very sad about this. That is another change made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a0066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 22:33:17.670645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-20 22:33:17.820404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-20 22:33:17.820427: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-20 22:33:17.852764: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-20 22:33:18.589830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-20 22:33:18.589970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-20 22:33:18.589982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbCallback\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import os\n",
    "\n",
    "data_path = \"~/aiffelthon/sample_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied cell\n",
    "def train():\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "\n",
    "    def seq_acc(y_true, y_pred):\n",
    "        y_bin=np.zeros_like(y_pred)\n",
    "        for i, dd in enumerate(y_bin):\n",
    "            for j in range(len(dd)):\n",
    "                pred=y_pred[i][j]\n",
    "                if pred>=0.5:\n",
    "                    y_bin[i][j]=1\n",
    "                else:\n",
    "                    y_bin[i][j]=0\n",
    "\n",
    "        predict_true = (y_true == y_bin)\n",
    "\n",
    "        try:\n",
    "            score = np.average(np.average(predict_true))\n",
    "        except ValueError:\n",
    "            score = mean_squared_error(y_true, y_bin)\n",
    "        return score\n",
    "\n",
    "    def my_seq_acc(y_true, y_pred):\n",
    "        score = tf.py_function(func=seq_acc, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_seq_acc') # tf 2.x\n",
    "        return score\n",
    "\n",
    "\n",
    "    class MySeqAccCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epochs, logs=None):\n",
    "            y_pred=self.model.predict(X_test)\n",
    "            print('sequence accuracy is {}'.format(seq_acc(y_test, y_pred)))\n",
    "\n",
    "\n",
    "    default_config={\n",
    "                         'seq_field':72,\n",
    "                         'stride_inside_seq':9,\n",
    "                         'stride_between_seqs':2,\n",
    "                         'learning_rate':0.01,\n",
    "                         'split_train_ratio':0.8,\n",
    "                         'epochs':20,\n",
    "                         'batch_size':64,\n",
    "                         'unit_GRU0':64}\n",
    "    ######### Wandb.init() ##########\n",
    "    wandb.init(config = default_config)\n",
    "\n",
    "    locations=['거문도', '울산', '거제도', '통영', '추자도']\n",
    "\n",
    "    df_merged=pd.read_csv(\"sensory_preprocessed_df_condition_labeled.csv\")\n",
    "    if df_merged.columns[0]=='Unnamed: 0':\n",
    "        df_merged = df_merged.iloc[:, 1:]\n",
    "\n",
    "    print('loaded dataset. Generating sequences')\n",
    "    seq_length=wandb.config.seq_field//wandb.config.stride_inside_seq\n",
    "    len_ds=len(df_merged)\n",
    "\n",
    "    seqs_idx=[]\n",
    "\n",
    "    start_idx=0\n",
    "    while start_idx<=len_ds-wandb.config.seq_field:\n",
    "        seqs_idx.append(list(range(start_idx, start_idx + wandb.config.seq_field, wandb.config.stride_inside_seq\n",
    "    )))\n",
    "        start_idx+=wandb.config.stride_between_seqs\n",
    "\n",
    "\n",
    "    seqs_idx[100],len(seqs_idx[100])\n",
    "\n",
    "    df_merged.reset_index(inplace=True, drop=True)\n",
    "    print('Any missing values exist:', df_merged.isna().all().all())\n",
    "\n",
    "    ds_train_cols=df_merged\n",
    "    ds_train_cols.reset_index(inplace=True, drop=True)\n",
    "    print('train dataset columns:',ds_train_cols.columns)\n",
    "\n",
    "    seq_dataset=np.zeros([len(seqs_idx), len(seqs_idx[0]), len(ds_train_cols.columns)])\n",
    "\n",
    "    for i, seq in enumerate(seqs_idx):\n",
    "        for j, row_number in enumerate(seq):\n",
    "            seq_dataset[i, j]=ds_train_cols.loc[row_number].to_numpy()\n",
    "\n",
    "    def not_bin_in_occurence(x):\n",
    "        if x==1 or x==0:\n",
    "            return x\n",
    "        else:\n",
    "            print('exceptional value(not 0 or 1) found. replaced by near one.')\n",
    "            if x>=0.5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    ds_train_cols['적조발생(조건)']=ds_train_cols['적조발생(조건)'].apply(not_bin_in_occurence)\n",
    "\n",
    "\n",
    "    split_index=int(len(seq_dataset)*wandb.config.split_train_ratio)\n",
    "    print(split_index, len(seq_dataset))\n",
    "\n",
    "    train_xy=seq_dataset[:split_index]\n",
    "    np.random.shuffle(train_xy)\n",
    "    X_train=train_xy[:,:,0:-1]\n",
    "    y_train=train_xy[:,:,-1]\n",
    "\n",
    "    test_xy=seq_dataset[split_index:]\n",
    "    np.random.shuffle(test_xy)\n",
    "    X_test=test_xy[:,:,0:-1]\n",
    "    y_test=test_xy[:,:,-1]\n",
    "\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape,'\\n\\n')\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(seq_length, 25)),\n",
    "        keras.layers.GRU(wandb.config.unit_GRU0),\n",
    "        keras.layers.Dense(seq_length, activation=\"sigmoid\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=wandb.config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "    \n",
    "    ######### WandbCallback ##########\n",
    "    model.fit(X_train, y_train,\n",
    "            batch_size=wandb.config.batch_size,\n",
    "            epochs=wandb.config.epochs, \n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[WandbCallback(training_data = (X_train, y_train),\n",
    "                                     validation_data = (X_test, y_test)), MySeqAccCallback()])\n",
    "    y_pred=model.predict(X_test)\n",
    "    \n",
    "    ######### Wandb.log() ##########\n",
    "    wandb.log({\"ValidationAcc\":seq_acc(y_test, y_pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcfa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
