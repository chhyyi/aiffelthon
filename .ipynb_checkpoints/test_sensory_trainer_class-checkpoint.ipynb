{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d695e3",
   "metadata": {},
   "source": [
    "# Sensory train class(Working on)\n",
    "I've made many versions for different setup and different data split and interpolation. Now it's time to merge them... but with some differences.  \n",
    "It will be subclass of tensorflow model (or Keras model?) with methods & initialization; \n",
    "- interpolation : interpolate missing values or not\n",
    "- limit dates range : to test & compile..    \n",
    "- change dataset: I have 3 different version of same dataset. Finally I have to use them...\n",
    "- sequence & model: unlike previous notebooks, it does not make long sequences from none-missing dataframe anymore. instead, it will just try to make sequence from existing rows.  \n",
    "- sensor positions: It will come with options to one-hot encode position or not.\n",
    "- normalization of mean, standard deviation\n",
    "- ready to be used by Weight and Bias sweep agent\n",
    "\n",
    "## Why does it changed (from previous version)\n",
    "previously I received 80/20 and 50/50 split dataset from our . But I made merged version instead of using that data. I have merged, interpolated missing values and merged 5 points data measured at same time. But all this efforts are not adquete to apply to the new dataset, which is separated randomly without considering date&time at all. So I had to do it everything again to compare new results with previous dataset. I cannot apply the previous method so I have to make new method and apply it to the both dataset. So I feel very sad about this. That is another change made.\n",
    "\n",
    "## Temporally stopped.\n",
    "Am I too unflexible? I've found a new idea to apply same method. I can interpolate missing values separately in train and test set! Why not? So it's stopped.  \n",
    "But how do we interpret that result? I don't know but who cares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae58fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "882bd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset as '/aiffel/aiffel/aiffelthon/sample_data/(class1 실측_70)_train.csv' and test dataset as '(class1 실측_30)_test.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c53a917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset as '/aiffel/aiffel/aiffelthon/sample_data/(class1 실측_70)_train.csv' and test dataset as '/aiffel/aiffel/aiffelthon/sample_data/(class1 실측_30)_test.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = os.path.join(os.getenv(\"HOME\"),\"aiffel/aiffelthon/sample_data\")\n",
    "dataset_train_split=0.7 #0.8, 0.7, 0.5\n",
    "dataset_class=1 #0 for no class. this is just arbitrary number between team members\n",
    "dataset_label_type='observe' # 'observe' or 'condition'\n",
    "\n",
    "list_data=os.listdir(data_path)\n",
    "if dataset_train_split==0.7 and dataset_class==1:\n",
    "    if dataset_label_type=='condition':\n",
    "        path_train=list_data[0]\n",
    "        path_test=list_data[1]\n",
    "    elif dataset_label_type=='observe':\n",
    "        path_train=list_data[5]\n",
    "        path_test=list_data[7]\n",
    "    else:\n",
    "        raise Exception(\"dataset_label_type should be 'observe' or 'condition'\")\n",
    "elif dataset_train_split==0.8 and dataset_class==0:\n",
    "    path_train=list_data[2]\n",
    "    path_test=list_data[3]\n",
    "elif dataset_train_split==0.5 and dataset_class==0:\n",
    "    path_train=list_data[4]\n",
    "    path_test=list_data[6]\n",
    "else:\n",
    "    raise Exception(\"Failed to initialize data files location.\")\n",
    "\n",
    "path_train=os.path.join(data_path,path_train)\n",
    "path_test=os.path.join(data_path,path_test)\n",
    "\n",
    "print(\"train dataset as '{}' and test dataset as '{}'\".format(path_train, path_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970db0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['인덱스', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)',\n",
      "       '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['인덱스', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)',\n",
      "       '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)',\n",
      "       '강수량(mm)', '적조발생(실측)', '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)',\n",
      "       '강수량(mm)', '적조발생(실측)', '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)',\n",
      "       '강수량(mm)', '적조발생(실측)', '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['인덱스', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)',\n",
      "       '적조발생(실측)'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)',\n",
      "       '강수량(mm)', '적조발생(실측)', '적조발생(조건)'],\n",
      "      dtype='object')\n",
      "Index(['인덱스', '지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)',\n",
      "       '적조발생(실측)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in os.listdir(data_path):\n",
    "    ds=pd.read_csv(os.path.join(data_path, file), encoding='euc-kr')\n",
    "    print(ds.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5436b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=pd.read_csv(path_train, encoding='euc-kr')\n",
    "test_ds=pd.read_csv(path_test, encoding='euc-kr')\n",
    "\n",
    "condition_cols=['지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)','적조발생(조건)']\n",
    "observe_cols=['지점', '일시', '풍속(m/s)', '풍향(deg)', '기온(°C)', '수온(°C)', '강수량(mm)','적조발생(실측)']\n",
    "\n",
    "if dataset_label_type=='condition':\n",
    "    train_ds=train_ds[condition_cols]\n",
    "    test_ds=test_ds[condition_cols]\n",
    "elif dataset_label_type=='observe':\n",
    "    train_ds=train_ds[observe_cols]\n",
    "    test_ds=test_ds[observe_cols]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6539f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>지점</th>\n",
       "      <th>일시</th>\n",
       "      <th>풍속(m/s)</th>\n",
       "      <th>풍향(deg)</th>\n",
       "      <th>기온(°C)</th>\n",
       "      <th>수온(°C)</th>\n",
       "      <th>강수량(mm)</th>\n",
       "      <th>적조발생(실측)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>통영</td>\n",
       "      <td>2020-09-08 15:00</td>\n",
       "      <td>6.2</td>\n",
       "      <td>219.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>울산</td>\n",
       "      <td>2021-08-12 7:00</td>\n",
       "      <td>3.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>27.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>거제도</td>\n",
       "      <td>2019-08-31 3:00</td>\n",
       "      <td>2.3</td>\n",
       "      <td>302.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>거문도</td>\n",
       "      <td>2021-08-12 21:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>울산</td>\n",
       "      <td>2020-10-25 11:00</td>\n",
       "      <td>7.3</td>\n",
       "      <td>295.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166518</th>\n",
       "      <td>추자도</td>\n",
       "      <td>2018-07-15 7:00</td>\n",
       "      <td>1.7</td>\n",
       "      <td>67.0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166519</th>\n",
       "      <td>통영</td>\n",
       "      <td>2020-04-14 12:00</td>\n",
       "      <td>2.4</td>\n",
       "      <td>265.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166520</th>\n",
       "      <td>통영</td>\n",
       "      <td>2017-12-23 10:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>317.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166521</th>\n",
       "      <td>통영</td>\n",
       "      <td>2019-11-15 21:00</td>\n",
       "      <td>6.2</td>\n",
       "      <td>235.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166522</th>\n",
       "      <td>울산</td>\n",
       "      <td>2018-06-22 11:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166523 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         지점                일시  풍속(m/s)  풍향(deg)  기온(°C)  수온(°C)  강수량(mm)  \\\n",
       "0        통영  2020-09-08 15:00      6.2    219.0    25.1    24.4      0.0   \n",
       "1        울산   2021-08-12 7:00      3.4     55.0    28.4    27.7      0.0   \n",
       "2       거제도   2019-08-31 3:00      2.3    302.0    23.7    24.6      0.0   \n",
       "3       거문도  2021-08-12 21:00      6.1    113.0    27.4    25.8      0.5   \n",
       "4        울산  2020-10-25 11:00      7.3    295.0    16.1    21.7      0.0   \n",
       "...     ...               ...      ...      ...     ...     ...      ...   \n",
       "166518  추자도   2018-07-15 7:00      1.7     67.0    23.1    22.3      0.0   \n",
       "166519   통영  2020-04-14 12:00      2.4    265.0    12.9    14.6      0.0   \n",
       "166520   통영  2017-12-23 10:00      5.1    317.0     9.7    12.8      0.0   \n",
       "166521   통영  2019-11-15 21:00      6.2    235.0    15.7    17.4      0.0   \n",
       "166522   울산  2018-06-22 11:00      2.0    238.0    23.9    22.1      0.0   \n",
       "\n",
       "        적조발생(실측)  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "166518         0  \n",
       "166519         0  \n",
       "166520         0  \n",
       "166521         0  \n",
       "166522         0  \n",
       "\n",
       "[166523 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45d65dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['year']=ds['일시'].apply(lambda x: pd.Timestamp(x).year)\n",
    "ds['mm']=ds['일시'].apply(lambda x: pd.Timestamp(x).month)\n",
    "ds['dd']=ds['일시'].apply(lambda x: pd.Timestamp(x).day)\n",
    "ds['hh']=ds['일시'].apply(lambda x: pd.Timestamp(x).hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied cell\n",
    "def train():\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "\n",
    "    def seq_acc(y_true, y_pred):\n",
    "        y_bin=np.zeros_like(y_pred)\n",
    "        for i, dd in enumerate(y_bin):\n",
    "            for j in range(len(dd)):\n",
    "                pred=y_pred[i][j]\n",
    "                if pred>=0.5:\n",
    "                    y_bin[i][j]=1\n",
    "                else:\n",
    "                    y_bin[i][j]=0\n",
    "\n",
    "        predict_true = (y_true == y_bin)\n",
    "\n",
    "        try:\n",
    "            score = np.average(np.average(predict_true))\n",
    "        except ValueError:\n",
    "            score = mean_squared_error(y_true, y_bin)\n",
    "        return score\n",
    "\n",
    "    def my_seq_acc(y_true, y_pred):\n",
    "        score = tf.py_function(func=seq_acc, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_seq_acc') # tf 2.x\n",
    "        return score\n",
    "\n",
    "\n",
    "    class MySeqAccCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epochs, logs=None):\n",
    "            y_pred=self.model.predict(X_test)\n",
    "            print('sequence accuracy is {}'.format(seq_acc(y_test, y_pred)))\n",
    "\n",
    "\n",
    "    default_config={\n",
    "                         'seq_field':72,\n",
    "                         'stride_inside_seq':9,\n",
    "                         'stride_between_seqs':2,\n",
    "                         'learning_rate':0.01,\n",
    "                         'split_train_ratio':0.8,\n",
    "                         'epochs':20,\n",
    "                         'batch_size':64,\n",
    "                         'unit_GRU0':64}\n",
    "    ######### Wandb.init() ##########\n",
    "    wandb.init(config = default_config)\n",
    "\n",
    "    locations=['거문도', '울산', '거제도', '통영', '추자도']\n",
    "\n",
    "    df_merged=pd.read_csv(\"sensory_preprocessed_df_condition_labeled.csv\")\n",
    "    if df_merged.columns[0]=='Unnamed: 0':\n",
    "        df_merged = df_merged.iloc[:, 1:]\n",
    "\n",
    "    print('loaded dataset. Generating sequences')\n",
    "    seq_length=wandb.config.seq_field//wandb.config.stride_inside_seq\n",
    "    len_ds=len(df_merged)\n",
    "\n",
    "    seqs_idx=[]\n",
    "\n",
    "    start_idx=0\n",
    "    while start_idx<=len_ds-wandb.config.seq_field:\n",
    "        seqs_idx.append(list(range(start_idx, start_idx + wandb.config.seq_field, wandb.config.stride_inside_seq\n",
    "    )))\n",
    "        start_idx+=wandb.config.stride_between_seqs\n",
    "\n",
    "\n",
    "    seqs_idx[100],len(seqs_idx[100])\n",
    "\n",
    "    df_merged.reset_index(inplace=True, drop=True)\n",
    "    print('Any missing values exist:', df_merged.isna().all().all())\n",
    "\n",
    "    ds_train_cols=df_merged\n",
    "    ds_train_cols.reset_index(inplace=True, drop=True)\n",
    "    print('train dataset columns:',ds_train_cols.columns)\n",
    "\n",
    "    seq_dataset=np.zeros([len(seqs_idx), len(seqs_idx[0]), len(ds_train_cols.columns)])\n",
    "\n",
    "    for i, seq in enumerate(seqs_idx):\n",
    "        for j, row_number in enumerate(seq):\n",
    "            seq_dataset[i, j]=ds_train_cols.loc[row_number].to_numpy()\n",
    "\n",
    "    def not_bin_in_occurence(x):\n",
    "        if x==1 or x==0:\n",
    "            return x\n",
    "        else:\n",
    "            print('exceptional value(not 0 or 1) found. replaced by near one.')\n",
    "            if x>=0.5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    ds_train_cols['적조발생(조건)']=ds_train_cols['적조발생(조건)'].apply(not_bin_in_occurence)\n",
    "\n",
    "\n",
    "    split_index=int(len(seq_dataset)*wandb.config.split_train_ratio)\n",
    "    print(split_index, len(seq_dataset))\n",
    "\n",
    "    train_xy=seq_dataset[:split_index]\n",
    "    np.random.shuffle(train_xy)\n",
    "    X_train=train_xy[:,:,0:-1]\n",
    "    y_train=train_xy[:,:,-1]\n",
    "\n",
    "    test_xy=seq_dataset[split_index:]\n",
    "    np.random.shuffle(test_xy)\n",
    "    X_test=test_xy[:,:,0:-1]\n",
    "    y_test=test_xy[:,:,-1]\n",
    "\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape,'\\n\\n')\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=(seq_length, 25)),\n",
    "        keras.layers.GRU(wandb.config.unit_GRU0),\n",
    "        keras.layers.Dense(seq_length, activation=\"sigmoid\"),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=wandb.config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "    \n",
    "    ######### WandbCallback ##########\n",
    "    model.fit(X_train, y_train,\n",
    "            batch_size=wandb.config.batch_size,\n",
    "            epochs=wandb.config.epochs, \n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=[WandbCallback(training_data = (X_train, y_train),\n",
    "                                     validation_data = (X_test, y_test)), MySeqAccCallback()])\n",
    "    y_pred=model.predict(X_test)\n",
    "    \n",
    "    ######### Wandb.log() ##########\n",
    "    wandb.log({\"ValidationAcc\":seq_acc(y_test, y_pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb6f82",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
